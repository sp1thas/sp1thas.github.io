[{"content":"Context A beta version of your awesome open-source software has been released. Great news so far. Except from introducing new features, fixing bugs and answering users' questions, it\u0026rsquo;s really important to have an estimation of the total number of users. In case you are not the only user of your OSS, it\u0026rsquo;s significant to monitor the active users for the following reasons:\n Motivation. Knowing that people are using you software will motivate you to keep maintaining and improving your project. Trustworthy software. Spreading the total number of active users indicates that your OSS is trustworthy.  So, this article is going to present a way to count active users and installations using Github Actions.\nNumerous package distributors Nowadays, there are numerous ways to distribute your OSS. Snapcraft and Flatpak are two of the most known package management tools. Have in mind that you always have to provide the option to \u0026ldquo;Build from source\u0026rdquo; to keep your geek users happy too. In a nutshell, you have to provide various installation methods, therefore, you have to summarize active users from more than one sources.\nCount installations So, let\u0026rsquo;s dig into how to count installations and active users for each installation method.\nKickstart script A common way to distribute your OSS is by providing a kickstart shell script, the purpose of this script is to download, build and install the software locally. In the case of dropboxignore, there is a kickstart script, and the installation instructions look like:\n   Method Command     curl sudo sh -c \u0026ldquo;$(curl -fsSL https://rb.gy/12plgs)\u0026quot; c   wget sudo sh -c \u0026ldquo;$(wget -qO- https://rb.gy/12plgs)\u0026quot; w    as you can see, I\u0026rsquo;m using and extra parameter at the end of each command in order to specify which lib was used to download the script (via wget or curl)\nInside the kickstart file, when installation is completed, I\u0026rsquo;m making few HTTP requests in order to count the installation process that just finished:\nINSTALL_COUNT_URL=\u0026#34;https://api.countapi.xyz/hit/dropboxignore.simakis.me\u0026#34; if [ \u0026#34;$0\u0026#34; == c ]; then curl -s --request GET --url \u0026#34;${INSTALL_COUNT_URL}/wget\u0026#34; \u0026gt; /dev/null curl -s --request GET --url \u0026#34;${INSTALL_COUNT_URL}/total\u0026#34; \u0026gt; /dev/null elif [ \u0026#34;$0\u0026#34; == w ]; then wget -q \u0026#34;${INSTALL_COUNT_URL}/curl\u0026#34; -O /dev/null 2\u0026gt; /dev/null wget -q \u0026#34;${INSTALL_COUNT_URL}/total\u0026#34; -O /dev/null 2\u0026gt; /dev/null fi CountAPI is a simple way to store stateful information like the total number of installations. You have to create a namespace for your project, and after that, make an GET /hit/:namespace?/:key to increase total installations by one.\nFinally, you can get the total number of installations by making an GET /info/:namespace?/:key. In case of dropboxignore:\n$ MANUAL_INSTALLATIONS=$(curl -s https://api.countapi.xyz/info/dropboxignore.simakis.me/total | jq -r .value) $ echo $MANUAL_INSTALLATIONS 12 At the time of writing dropboxignore has been installed 12 times using the kickstart.\nSnapcraft Another common way to distribute your software is as a snap. Snapcraft provides a nice monitoring dashboard:\nBut in our case, this is not enough, we have to extract the raw data programmatically. For this reason we are going to use Snapcraft CLI and Snapcraft Action\nTo get snapcraft metrics, you have to export a credential file and to create a github secret using the content of the exported file:\n$ snapcraft login $ snapcraft export-login exported In order to get the current total number of active user, you have to use the snapcraft metric command:\n$ SNAP_INSTALLATIONS=$( \\  snapcraft metrics dropboxignore --name installed_base_by_channel \\  --start \u0026#34;$(date +\u0026#34;%Y-%m-%d\u0026#34; --date=\u0026#34;yesterday\u0026#34;)\u0026#34; \\  --end \u0026#34;$(date +\u0026#34;%Y-%m-%d\u0026#34; --date=\u0026#34;yesterday\u0026#34;)\u0026#34; \\  --format=json \\  | jq \u0026#39;.series[].values[]\u0026#39; \\  | awk \u0026#39;{s+=$1} END {printf \u0026#34;%.0f\\n\u0026#34;, s}\u0026#39; \\ ) $ echo $SNAP_INSTALLATIONS 118 So, 118 users have installed dropboxignore using snap.\nSummarize The easy part is to summarize user from different sources:\n$ TOTAL_INSTALLATIONS=$((MANUAL_INSTALLATIONS + SNAP_INSTALLATIONS)) $ echo $TOTAL_INSTALLATIONS 130 130 users. I\u0026rsquo;m already boosted morally ぃ.\nAutomate the boring staff using Github Actions Finally, given that we retrieved the necessary information programmatically, we are going to create a nightly job to store active users. The wiki of the repo should be enabled, so, we will use the wiki to store the statistics.\nJSON_BADGE_STRING=$(cat \u0026lt;\u0026lt;-END {\u0026#34;schemaVersion\u0026#34;: 1, \u0026#34;label\u0026#34;: \u0026#34;installations\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;$TOTAL_INSTALLATIONS\u0026#34;} END ) JSON_FULL_STRING=$(cat \u0026lt;\u0026lt;-END {\u0026#34;manual-installations\u0026#34;: \u0026#34;$MANUAL_INSTALLATIONS\u0026#34;, \u0026#34;snap-installations\u0026#34;: \u0026#34;$SNAP_INSTALLATIONS\u0026#34;, \u0026#34;total-installations\u0026#34;: \u0026#34;$TOTAL_INSTALLATIONS\u0026#34;} END ) FILENAME=\u0026#34;$(date +\u0026#34;%Y-%m-%d\u0026#34; --date=\u0026#34;yesterday\u0026#34;)-stats.json\u0026#34; echo \u0026#34;$JSON_BADGE_STRING\u0026#34; \u0026gt; latest-stats.json echo \u0026#34;$JSON_FULL_STRING\u0026#34; \u0026gt; \u0026#34;$FILENAME\u0026#34; git add latest-stats.json git add \u0026#34;$FILENAME\u0026#34; if ! git diff --staged --exit-code then git config --global user.email \u0026#34;coverage-comment-action\u0026#34; git config --global user.name \u0026#34;Coverage Comment Action\u0026#34; git commit -m \u0026#34;$(date +\u0026#34;%Y-%m-%d\u0026#34; --date=\u0026#34;yesterday\u0026#34;)stats\u0026#34; git push -u origin else echo \u0026#34;No change detected, skipping.\u0026#34; fi Badges everywhere There is always space for one more badge with the total number of Cronjob using Github Actions The number of active users is updated every night using the following job:\nname: Nightly stats on: schedule: - cron: \u0026#34;1 1 * * *\u0026#34; workflow_dispatch: jobs: stats: runs-on: ubuntu-latest steps: - name: Install Snapcraft uses: samuelmeuli/action-snapcraft@v1 with: snapcraft_token: ${{ secrets.SNAPCRAFT_LOGIN_FILE }} - name: Install os dependencies run: sudo apt update \u0026amp;\u0026amp; sudo apt install curl jq git - name: Create stats file run: sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/sp1thas/dropboxignore/master/utils/stats.sh)\u0026#34; \u0026#39;${{ secrets.GITHUB_TOKEN }}\u0026#39; Next steps  Count manual uninstalls. Count flatpak installations.  References  sp1thas/dropboxignore dropboxignore/utils/stats.sh dropboxignore/.github/workflows/stats.yml Snapcraft Action Snapcraft CountAPI  ","permalink":"https://simakis.me/posts/oss-user-counting/","summary":"Context A beta version of your awesome open-source software has been released. Great news so far. Except from introducing new features, fixing bugs and answering users' questions, it\u0026rsquo;s really important to have an estimation of the total number of users. In case you are not the only user of your OSS, it\u0026rsquo;s significant to monitor the active users for the following reasons:\n Motivation. Knowing that people are using you software will motivate you to keep maintaining and improving your project.","title":"OSS: Estimate and monitor the number of active users using Github Actions"},{"content":"This article demonstrates an easy way (hopefully) to add your prefect flow visualizations into your sphinx documentation using the sphinxcontrib-prefectviz plugin.\nSome context Let\u0026rsquo;s say that you have implemented some pipelines using the lovely Prefect, great news so far, you chose a great tool to automate the boring stuff. Now it\u0026rsquo;s time to document your flows, a simple way to have an overview of your flows is to use the flow visualization.\nLet\u0026rsquo;s start by creating an awesome project called example. The only prefect flow that this project has is inside the flow.py file:\nexample/flow.py:\nfrom prefect import task, Flow, Parameter @task def add(a: int, b: int) -\u0026gt; int: return a + b with Flow(name=\u0026#34;1+1=2\u0026#34;) as flow: a = Parameter(name=\u0026#34;a\u0026#34;, default=1) b = Parameter(name=\u0026#34;b\u0026#34;, default=1) add(a, b) and the flow visualization looks like:\nflow.visualize() Wouldn\u0026rsquo;t be nice to add this visualization into your Sphinx documentation with the minimum effort?\nThe Sphinx plugin sphinxcontrib-prefectviz aims to help you to add your prefect flow visualizations easily into you sphinx docs. As prefect users, we really want to automate whatever is automatable. Given that, let\u0026rsquo;s demonstrate this plugin using a simple Sphinx project.\nOur awesome example project now looks like:\nexample  _build  conf.py  index.rst  make.bat  Makefile  _static  _templates  flow.py A Sphinx project has been created in the project\u0026rsquo;s root folder.\nNow it\u0026rsquo;s time to install the Sphinx plugin:\npip install sphinxcontrib-prefectviz And add it into the Sphinx project:\nextensions = [\u0026#39;sphinxcontrib.prefectviz\u0026#39;] Last but not least, we have to ensure that flow.py can be imported by sphinx, to achieve such thing, we have to add the current path into the PYTHONPATH by adding the following LOC in the very top of conf.py:\nimport os import sys sys.path.insert(0, os.path.abspath(\u0026#39;.\u0026#39;)) Ready to use Now we are ready to use the plugin. Let\u0026rsquo;s add the flow visualization into the index.rst:\n.. flowviz:: flow.flowand voil! The flow visualization appears in the index page:\nFuture work Currently, the plugin consists of a single Sphinx directive, the goal is to add more directives in order to support more prefect-related stuff like:\n Add flow scheduling into Sphinx using a human-friendly format  Do you have an idea on this? Feel free to open an issue to discuss it.\n","permalink":"https://simakis.me/posts/sphinx-prefect/","summary":"This article demonstrates an easy way (hopefully) to add your prefect flow visualizations into your sphinx documentation using the sphinxcontrib-prefectviz plugin.\nSome context Let\u0026rsquo;s say that you have implemented some pipelines using the lovely Prefect, great news so far, you chose a great tool to automate the boring stuff. Now it\u0026rsquo;s time to document your flows, a simple way to have an overview of your flows is to use the flow visualization.","title":"Add prefect flow visualizations into a Sphinx project"},{"content":"The \u0026ldquo;Problem\u0026rdquo; A common issue/request about Dropbox was about ignoring files without using Selective Sync. Dropbox recently introduced the ability to ignore specific files or folders. Although the significance of the new feature, many users [ 1, 2 , 3, 4] have expressed the importance of ignoring files from Dropbox using Glob just like .gitignore. For various reasons Dropbox doesn\u0026rsquo;t seem to have the will to implement such a feature :/\nThe walk-around As a result, I decided to implement dropboxignore which is a simple shell script that ignores files from Dropbox based on glob patterns. Additionally, existing .gitignore files can be used to automatically generate .dropboxignore files.\nThe main difference between dropboxignore and the other relevant projects:\n rozbb/DropboxIgnore swapagarwal/dropbox_ignore ridvanaltun/dropbox-ignore-anywhere MichalKarol/dropboxignore mweirauch/dropignore  is the fact that dropboxignore is implemented in shell and has the minimum requirements (attr package is the only requirement)\ndropboxignore is currently available only for Mac and Linux and has various features:\n Ignore/Revert specific files Automatically update .dropboxignore Revert all ignored files Updated .dropboxignore based on changes in .gitignore file. Generate .dropboxignore files based on existing .gitignore files. Generate, update and ignore using a single command. Delete all .dropboxignore files.  A usecase The simplest usage example could be this: Let\u0026rsquo;s say that you want to ignore every .txt under your Dropbox folder, in that case you only have to run the following command:\n$ dropboxignore ignore *.txt You can find more usage examples here. The full documentation of the project is available here\nThings to not expect from dropboxignore  Automatically ignore new matching files.  When a new file has been created and is matched by a .dropboxignore file, user should re-ignore matching files. A cronjob could solve this drawback.\nAutomatically detect deletions from .gitignore files  Current implementation can detect and update .dropboxignore file if an extra pattern has been added in the corresponding .gitignores but it can not handle deletions in the same way.\n","permalink":"https://simakis.me/posts/exclude-files-using-dropboxignore/","summary":"The \u0026ldquo;Problem\u0026rdquo; A common issue/request about Dropbox was about ignoring files without using Selective Sync. Dropbox recently introduced the ability to ignore specific files or folders. Although the significance of the new feature, many users [ 1, 2 , 3, 4] have expressed the importance of ignoring files from Dropbox using Glob just like .gitignore. For various reasons Dropbox doesn\u0026rsquo;t seem to have the will to implement such a feature :/","title":"Exclude files from your dropbox easily using dropboxignore"},{"content":"Introduction What is the problem and how deal with it When it comes to image storing, a common pitfall is to save all the images in a single folder. If the number of images is less than few thousands, when, stop reading this post because you will not face any issue. On the other hand, if you are planing to store numerous images, then consider splitting them in different folders. Listing a directory will become faster, more efficient and at the end of the day, your kernel will be happier. A common pattern is to create a folder structure based on the name of every file. For example, lets say that path/to/image/dir will be the main directory, and you want to store imagefile.jpg. Create folder structure based on files characters and save the file inside the leaf folder:\n$ tree path/to/image/dir path/to/image/dir  i  m  a  imagefile.jpg A testcase Given the following situation:\n Folder structures depth is 3 The maximum number of images per leaf folder is 1000 A common hashing function is used for naming ([a-z0-9])  The main folder can host up to:\n$$\\left ( \\left ( 26 + 10 \\right )^3 \\right ) * 1000 \\approx 46 \\text{ Milion images}$$\nApplication Time to get our hands dirty.\nDescription The purpose of this post is demonstrate an easy way to apply this methodology using scrapy and specifically the ImagePipeline. The default behavior of ImagePipeline is to store all images in the same folder based on IMAGES_STOREs value in settings.py. We are going to make an ImagePipeline sub-class and we will override file_path and thumb_path methods. Please find below the full pipeline:\nImplementation  ","permalink":"https://simakis.me/posts/scrapy-store-images-efficiently-using-folder-structure/","summary":"Introduction What is the problem and how deal with it When it comes to image storing, a common pitfall is to save all the images in a single folder. If the number of images is less than few thousands, when, stop reading this post because you will not face any issue. On the other hand, if you are planing to store numerous images, then consider splitting them in different folders. Listing a directory will become faster, more efficient and at the end of the day, your kernel will be happier.","title":"Store images efficiently in scrapy using folder structure"},{"content":" ","permalink":"https://simakis.me/posts/semi-supervised-fraud-detection/","summary":" ","title":"Semi-Supervised Fraud Detection"},{"content":"kaggle dataset: Book Depository Dataset\nkaggle notebook: Introduction to Book Depository Dataset\ngithub repo: book-depository-dataset\nBook Depository Dataset EDA Through this notebook we will try to become familiar Book Depository Dataset and extract some usefull insights. The goal of this notebook is to become an introductory step for the dataset.\nimport pandas as pd import os import json from glob import glob import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline Dataset Structure Files:\n categories.csv dataset.csv formats.csv places.csv  The dataset consists of 5 file, the main dataset.csv file and some extra files. Extra files works as lookup tables for category, author, format and publication place. The reason behind this decision was to prevent data redundancy.\nFields:\n authors: Book\u0026rsquo;s author(s) (list of str) bestsellers-rank: Bestsellers ranking (int) categories: Book\u0026rsquo;s categories. Check authors.csv for mapping (list of int) description: Book description (str) dimension_x: Book\u0026rsquo;s dimension X (float cm) dimension_y: Book\u0026rsquo;s dimension Y (float cm) dimension_z: Book\u0026rsquo;s dimension Z (float mm) edition: Edition (str) edition-statement: Edition statement (str) for-ages: Range of ages (str) format: Book\u0026rsquo;s format. Check formats.csv for mapping (int) id: Book\u0026rsquo;s unique id (int) illustrations-note: imprint: index-date: Book\u0026rsquo;s crawling date (date) isbn10: Book\u0026rsquo;s ISBN-10 (str) isbn13: Book\u0026rsquo;s ISBN-13 (str) lang: List of book' language(s) publication-date: Publication date (date) publication-place: Publication place (id) publisher: Publisher (str) rating-avg: Rating average [0-5] (float) rating-count: Number of ratings title: Book\u0026rsquo;s title (str) url: Book relative url (https://bookdepository.com + url) weight: Book\u0026rsquo;s weight (float gr)  So, lets assign each file to a different dataframe\nif os.path.exists(\u0026#39;../input/book-depository-dataset\u0026#39;): path_prefix = \u0026#39;../input/book-depository-dataset/{}.csv\u0026#39; else: path_prefix = \u0026#39;../export/kaggle/{}.csv\u0026#39; df, df_f, df_a, df_c, df_p = [ pd.read_csv(path_prefix.format(_)) for _ in (\u0026#39;dataset\u0026#39;, \u0026#39;formats\u0026#39;, \u0026#39;authors\u0026#39;, \u0026#39;categories\u0026#39;, \u0026#39;places\u0026#39;) ] # df = df.sample(n=500) df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  authors bestsellers-rank categories description dimension-x dimension-y dimension-z edition edition-statement for-ages ... isbn10 isbn13 lang publication-date publication-place rating-avg rating-count title url weight     0 [1] 57858 [220, 233, 237, 2644, 2679, 2689] They were American and British air force offic... 142.0 211.0 20.0 NaN Reissue NaN ... 393325792 9.780393e+12 en 2004-08-17 1.0 4.24 6688.0 The Great Escape /Great-Escape-Paul-Brickhill/9780393325799 243.00   1 [2, 3] 114465 [235, 3386] John Moran and Carl Williams were the two bigg... 127.0 203.2 25.4 NaN NaN NaN ... 184454737X 9.781845e+12 en 2009-03-13 2.0 3.59 291.0 Underbelly : The Gangland War /Underbelly-Andrew-Rule/9781844547371 285.76   2 [4] 61,471 [241, 245, 247, 249, 378] Plain English is the art of writing clearly, c... 136.0 195.0 16.0 Revised 4th Revised edition NaN ... 199669171 9.780200e+12 en 2013-09-15 3.0 4.18 128.0 Oxford Guide to Plain English /Oxford-Guide-Plain-English-Martin-Cutts/97801... 338.00   3 [5] 1,347,994 [245, 253, 263, 273, 274, 276, 279, 280, 281, ... When travelling, do you want to journey off th... 136.0 190.0 33.0 Unabridged Unabridged edition NaN ... 1444185497 9.781444e+12 en 2014-12-03 2.0 NaN NaN Get Talking and Keep Talking Portuguese Total ... /Get-Talking-Keep-Talking-Portuguese-Total-Aud... 156.00   4 [6] 58154 [1938, 1941, 1995] No matter what your actual job title, you are-... 179.0 229.0 18.0 NaN NaN NaN ... 321934075 9.780322e+12 en 2016-02-28 4.0 4.30 212.0 The Truthful Art : Data, Charts, and Maps for ... /Truthful-Art-Alberto-Cairo/9780321934079 732.00    5 rows  25 columns\n Basic Stats Firtly, lets display some basic statistics:\ndf.describe()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  dimension-x dimension-y dimension-z id isbn13 publication-place rating-avg rating-count weight     count 742112.000000 713278.000000 742112.000000 7.790050e+05 7.658780e+05 556846.000000 502381.000000 5.023810e+05 714289.000000   mean 160.560373 222.289753 25.609538 9.781553e+12 9.781559e+12 247.989972 3.932002 1.187949e+04 444.768939   std 37.487785 43.145377 44.218401 1.563374e+09 1.565216e+09 643.253808 0.530740 1.174093e+05 610.212039   min 0.250000 1.000000 0.130000 9.771131e+12 9.780000e+12 1.000000 1.000000 1.000000e+00 15.000000   25% 135.000000 198.000000 9.140000 9.780764e+12 9.780772e+12 2.000000 3.690000 6.000000e+00 172.370000   50% 152.000000 229.000000 16.000000 9.781473e+12 9.781475e+12 8.000000 4.000000 5.200000e+01 299.000000   75% 183.000000 240.000000 25.000000 9.781723e+12 9.781724e+12 178.000000 4.220000 6.880000e+02 521.630000   max 1905.000000 1980.000000 1750.000000 9.798485e+12 9.798389e+12 5501.000000 5.000000 5.870281e+06 90717.530000     Publication Date Distribution: Most books are published in t\ndf[\u0026#34;publication-date\u0026#34;] = df[\u0026#34;publication-date\u0026#34;].astype(\u0026#34;datetime64\u0026#34;) df.groupby(df[\u0026#34;publication-date\u0026#34;].dt.year).id.count().plot(title=\u0026#39;Publication date distribution\u0026#39;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7827af68d0\u0026gt;  df[\u0026#34;index-date\u0026#34;] = df[\u0026#34;index-date\u0026#34;].astype(\u0026#34;datetime64\u0026#34;) df.groupby(df[\u0026#34;index-date\u0026#34;].dt.month).id.count().plot(title=\u0026#39;Crawling date distribution\u0026#39;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7827af61d0\u0026gt;  df.groupby([\u0026#39;lang\u0026#39;]).id.count().sort_values(ascending=False)[:5].plot(kind=\u0026#39;pie\u0026#39;, title=\u0026#34;Most common languages\u0026#34;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f78279aca58\u0026gt;  import math sns.lineplot(data=df.groupby(df[\u0026#39;rating-avg\u0026#39;].dropna().apply(int)).id.count().reset_index(), x=\u0026#39;rating-avg\u0026#39;, y=\u0026#39;id\u0026#39;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7827970dd8\u0026gt;  dims = pd.DataFrame({ \u0026#39;dims\u0026#39;: df[\u0026#39;dimension-x\u0026#39;].fillna(\u0026#39;0\u0026#39;).astype(int).astype(str).str.cat( df[\u0026#39;dimension-y\u0026#39;].fillna(\u0026#39;0\u0026#39;).astype(int).astype(str), sep=\u0026#34; x \u0026#34;).replace(\u0026#39;0 x 0\u0026#39;, \u0026#39;Unknown\u0026#39;).values, \u0026#39;id\u0026#39;: df[\u0026#39;id\u0026#39;].values }) dims.groupby([\u0026#39;dims\u0026#39;]).id.count().sort_values(ascending=False)[:8].plot(kind=\u0026#39;pie\u0026#39;, title=\u0026#34;Most common dimensions\u0026#34;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f77ee8a2b38\u0026gt;  pd.merge( df[[\u0026#39;id\u0026#39;, \u0026#39;publication-place\u0026#39;]], df_p, left_on=\u0026#39;publication-place\u0026#39;, right_on=\u0026#39;place_id\u0026#39; ).groupby([\u0026#39;place_name\u0026#39;]).id.count().sort_values(ascending=False)[:8].plot(kind=\u0026#39;pie\u0026#39;, title=\u0026#34;Most common publication places\u0026#34;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f77ee96a208\u0026gt;  ","permalink":"https://simakis.me/posts/introduction-to-book-depository-dataset/","summary":"kaggle dataset: Book Depository Dataset\nkaggle notebook: Introduction to Book Depository Dataset\ngithub repo: book-depository-dataset\nBook Depository Dataset EDA Through this notebook we will try to become familiar Book Depository Dataset and extract some usefull insights. The goal of this notebook is to become an introductory step for the dataset.\nimport pandas as pd import os import json from glob import glob import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline Dataset Structure Files:","title":"Book Depository Dataset EDA"},{"content":"           4/2022 Astronomer Certification for Apache Airflow Fundamentals cert Astronomer   3/2022 Introduction to Kube铿ow: Pipelines cert Arrikto   1/2022 Pulsar API Essentials - Python cert StreamNative   1/2022 Apache Pulsar Fundamentals cert StreamNative   1/2022 Introduction to Kubeflow: Notebooks cert Arrikto   1/2022 Open Source Software Development, Linux and Git Specialization cert Coursera   12/2021 Architecting with Google Kubernetes Engine Specialization cert Coursera   12/2021 Introduction to Kubeflow: Fundamentals cert Arrikto   12/2021 Introduction to Machine Learning in Production cert Coursera   10/2020 AWS Fundamentals Specialization cert Coursera   4/2020 Programming Foundations: Web Security cert LinkedIn Learning   2/2020 Applied Text Mining in Python cert Coursera   2/2020 Sequences, Time Series and Prediction cert Coursera   3/2019 Oe\\n conference cert Athens, Greece   12/2017 Spark and Python for Big Data with PySpark cert Udemy   11/2017 2th DevOps Conference 2017 cert Athens, Greece   11/2017 0th Conference of Free \u0026amp; Open Source Software Communities  Athens, Greece   4/2017 Solr 101 cert Cognitive Class   5/2011 4th Conference of Free \u0026amp; Open Source Software Communities  Patras, Greece    ","permalink":"https://simakis.me/certifications/","summary":"           4/2022 Astronomer Certification for Apache Airflow Fundamentals cert Astronomer   3/2022 Introduction to Kube铿ow: Pipelines cert Arrikto   1/2022 Pulsar API Essentials - Python cert StreamNative   1/2022 Apache Pulsar Fundamentals cert StreamNative   1/2022 Introduction to Kubeflow: Notebooks cert Arrikto   1/2022 Open Source Software Development, Linux and Git Specialization cert Coursera   12/2021 Architecting with Google Kubernetes Engine Specialization cert Coursera   12/2021 Introduction to Kubeflow: Fundamentals cert Arrikto   12/2021 Introduction to Machine Learning in Production cert Coursera   10/2020 AWS Fundamentals Specialization cert Coursera   4/2020 Programming Foundations: Web Security cert LinkedIn Learning   2/2020 Applied Text Mining in Python cert Coursera   2/2020 Sequences, Time Series and Prediction cert Coursera   3/2019 Oe\\n conference cert Athens, Greece   12/2017 Spark and Python for Big Data with PySpark cert Udemy   11/2017 2th DevOps Conference 2017 cert Athens, Greece   11/2017 0th Conference of Free \u0026amp; Open Source Software Communities  Athens, Greece   4/2017 Solr 101 cert Cognitive Class   5/2011 4th Conference of Free \u0026amp; Open Source Software Communities  Patras, Greece    ","title":"Certifications \u0026 Conferences"},{"content":"Feel free to drop a line using the following contact form.\n Send     grecaptcha.ready(function() { grecaptcha.execute('6Lfw0jEeAAAAAG9AukBmzaeHtY8X1cdr8MnGclFU', {action: 'homepage'}) .then(function(token) { document.getElementById('captchaResponse').value = token; }); });   ","permalink":"https://simakis.me/contact/","summary":"Feel free to drop a line using the following contact form.\n Send     grecaptcha.ready(function() { grecaptcha.execute('6Lfw0jEeAAAAAG9AukBmzaeHtY8X1cdr8MnGclFU', {action: 'homepage'}) .then(function(token) { document.getElementById('captchaResponse').value = token; }); });   ","title":"Contact form"},{"content":"Click here to download my CV.\n  #the-canvas { border: 1px solid black; direction: ltr; width: 100%; height: auto; display: none; } #paginator { display: none; text-align: center; margin-bottom: 10px; } #loadingWrapper { display: none; justify-content: center; align-items: center; width: 100%; height: 350px; } #loading { display: inline-block; width: 50px; height: 50px; border: 3px solid #d2d0d0;; border-radius: 50%; border-top-color: #383838; animation: spin 1s ease-in-out infinite; -webkit-animation: spin 1s ease-in-out infinite; } @keyframes spin { to { -webkit-transform: rotate(360deg); } } @-webkit-keyframes spin { to { -webkit-transform: rotate(360deg); } }  Previous Next \u0026nbsp; \u0026nbsp; Page:  /       window.onload = function() { var url = \"https:\\/\\/simakis.me\\/\" + '\\/cv.pdf'; var hidePaginator = \"\" === \"true\"; var hideLoader = \"\" === \"true\"; var selectedPageNum = parseInt(\"\") || 1; var pdfjsLib = window['pdfjs-dist/build/pdf']; pdfjsLib.GlobalWorkerOptions.workerSrc = \"https:\\/\\/simakis.me\\/\" + '/js/pdf-js/build/pdf.worker.js'; var pdfDoc = null, pageNum = selectedPageNum, pageRendering = false, pageNumPending = null, scale = 3, canvas = document.getElementById('the-canvas'), ctx = canvas.getContext('2d'), paginator = document.getElementById(\"paginator\"), loadingWrapper = document.getElementById('loadingWrapper'); showPaginator(); showLoader(); function renderPage(num) { pageRendering = true; pdfDoc.getPage(num).then(function(page) { var viewport = page.getViewport({scale: scale}); canvas.height = viewport.height; canvas.width = viewport.width; var renderContext = { canvasContext: ctx, viewport: viewport }; var renderTask = page.render(renderContext); renderTask.promise.then(function() { pageRendering = false; showContent(); if (pageNumPending !== null) { renderPage(pageNumPending); pageNumPending = null; } }); }); document.getElementById('page_num').textContent = num; } function showContent() { loadingWrapper.style.display = 'none'; canvas.style.display = 'block'; } function showLoader() { if(hideLoader) return loadingWrapper.style.display = 'flex'; canvas.style.display = 'none'; } function showPaginator() { if(hidePaginator) return paginator.style.display = 'block'; } function queueRenderPage(num) { if (pageRendering) { pageNumPending = num; } else { renderPage(num); } } function onPrevPage() { if (pageNum = pdfDoc.numPages) { return; } pageNum++; queueRenderPage(pageNum); } document.getElementById('next').addEventListener('click', onNextPage); pdfjsLib.getDocument(url).promise.then(function(pdfDoc_) { pdfDoc = pdfDoc_; var numPages = pdfDoc.numPages; document.getElementById('page_count').textContent = numPages; if(pageNum  numPages) { pageNum = numPages } renderPage(pageNum); }); }  ","permalink":"https://simakis.me/cv/","summary":"Click here to download my CV.\n  #the-canvas { border: 1px solid black; direction: ltr; width: 100%; height: auto; display: none; } #paginator { display: none; text-align: center; margin-bottom: 10px; } #loadingWrapper { display: none; justify-content: center; align-items: center; width: 100%; height: 350px; } #loading { display: inline-block; width: 50px; height: 50px; border: 3px solid #d2d0d0;; border-radius: 50%; border-top-color: #383838; animation: spin 1s ease-in-out infinite; -webkit-animation: spin 1s ease-in-out infinite; } @keyframes spin { to { -webkit-transform: rotate(360deg); } } @-webkit-keyframes spin { to { -webkit-transform: rotate(360deg); } }  Previous Next \u0026nbsp; \u0026nbsp; Page:  /       window.","title":"Curriculum Vitae"},{"content":" great-expectations/great_expectations:  #5393 路 [FEATURE] Support underscore in the condition_value of a row_condition   sphinx-contrib/prefectviz: Maintainer nltk/nltk: #2910 路 Fix FileNotFoundError when the download_dir is a non-existing nested folder jazzband/django-auditlog: PR #295 路 Replace deprecated smart_text() with smart_str() PrefectHQ/prefect:  #5541 路 Feat/Support default value on DatetimeParameter #3729 路 Support passing helper_script to ShellTask/DBTShellTask at runtime #3612 路 pass day_or croniter argument to CronClock \u0026amp; CronSchedule #3548 路 RenameFlowRunTask: use default flow_run_id value from context #3522 路 specify slack webhook_secret on runtime #3479 路 explicit exception chaining for spacy tasks #3460 路 S3List filtering using the LastModified  #3072 路 Ignore calls to flow.register when parsing a flow using file based storage #2924 路 Fix ValueError on flow.replace   boudinfl/pke: #121 路 disable unnecessary spacy pipeline components dldl/sphinx-server: #35 路 Support Python 3 bisohns/search-engine-parser:  #64 路 Added Coursera Search Engine #52 路 Added Google Scholar   erichulser/mkdocs_autodoc: #3 路 add missing dependency twingly/twingly-search-api-python: #40 路 Add all required modules in setup.py lorien/awesome-web-scraping:  #65 路 add rq #64 路 add Requestium #40 路 add grequests   lk-geimfari/expynent:  #69 路 add pattern and test for greek license plates #57 路 Pattern for greek phones   nltk/nltk_data: #103 路 Add Greek Keywords riseupnet/riseup_help:  #411 路 \u0026ldquo;Riseup chat\u0026rdquo; has been translated to greek #403 路 Greek translation for the home page    ","permalink":"https://simakis.me/contributions/","summary":"great-expectations/great_expectations:  #5393 路 [FEATURE] Support underscore in the condition_value of a row_condition   sphinx-contrib/prefectviz: Maintainer nltk/nltk: #2910 路 Fix FileNotFoundError when the download_dir is a non-existing nested folder jazzband/django-auditlog: PR #295 路 Replace deprecated smart_text() with smart_str() PrefectHQ/prefect:  #5541 路 Feat/Support default value on DatetimeParameter #3729 路 Support passing helper_script to ShellTask/DBTShellTask at runtime #3612 路 pass day_or croniter argument to CronClock \u0026amp; CronSchedule #3548 路 RenameFlowRunTask: use default flow_run_id value from context #3522 路 specify slack webhook_secret on runtime #3479 路 explicit exception chaining for spacy tasks #3460 路 S3List filtering using the LastModified  #3072 路 Ignore calls to flow.","title":"My F/OSS Contributions"}]