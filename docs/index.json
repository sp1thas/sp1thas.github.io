[{"content":"This article demonstrates an easy way (hopefully) to add your prefect flow visualizations into your sphinx documentation using the sphinxcontrib-prefectviz plugin.\nSome context Let\u0026rsquo;s say that you have implemented some pipelines using the lovely Prefect, great news so far, you chose a great tool to automate the boring stuff. Now it\u0026rsquo;s time to document your flows, a simple way to have an overview of your flows is to use the flow visualization.\nLet\u0026rsquo;s start by creating an awesome project called example. The only prefect flow that this project has is inside the flow.py file:\nexample/flow.py:\nfrom prefect import task, Flow, Parameter @task def add(a: int, b: int) -\u0026gt; int: return a + b with Flow(name=\u0026#34;1+1=2\u0026#34;) as flow: a = Parameter(name=\u0026#34;a\u0026#34;, default=1) b = Parameter(name=\u0026#34;b\u0026#34;, default=1) add(a, b) and the flow visualization looks like:\nflow.visualize() Wouldn\u0026rsquo;t be nice to add this visualization into your Sphinx documentation with the minimum effort?\nThe Sphinx plugin sphinxcontrib-prefectviz aims to help you to add your prefect flow visualizations easily into you sphinx docs. As prefect users, we really want to automate whatever is automatable. Given that, let\u0026rsquo;s demonstrate this plugin using a simple Sphinx project.\nOur awesome example project now looks like:\nexample ├── _build ├── conf.py ├── index.rst ├── make.bat ├── Makefile ├── _static ├── _templates └── flow.py A Sphinx project has been created in the project\u0026rsquo;s root folder.\nNow it\u0026rsquo;s time to install the Sphinx plugin:\npip install sphinxcontrib-prefectviz And add it into the Sphinx project:\nextensions = [\u0026#39;sphinxcontrib.prefectviz\u0026#39;] Last but not least, we have to ensure that flow.py can be imported by sphinx, to achieve such thing, we have to add the current path into the PYTHONPATH by adding the following LOC in the very top of conf.py:\nimport os import sys sys.path.insert(0, os.path.abspath(\u0026#39;.\u0026#39;)) Ready to use Now we are ready to use the plugin. Let\u0026rsquo;s add the flow visualization into the index.rst:\n.. flowviz:: flow.flowand voilà! The flow visualization appears in the index page:\nFuture work Currently, the plugin consists of a single Sphinx directive, the goal is to add more directives in order to support more prefect-related stuff like:\n Add flow scheduling into Sphinx using a human-friendly format  Do you have an idea on this? Feel free to open an issue to discuss it.\n","permalink":"https://simakis.me/posts/sphinx-prefect/","summary":"This article demonstrates an easy way (hopefully) to add your prefect flow visualizations into your sphinx documentation using the sphinxcontrib-prefectviz plugin.\nSome context Let\u0026rsquo;s say that you have implemented some pipelines using the lovely Prefect, great news so far, you chose a great tool to automate the boring stuff. Now it\u0026rsquo;s time to document your flows, a simple way to have an overview of your flows is to use the flow visualization.","title":"Add prefect flow visualizations into a Sphinx project"},{"content":"The \u0026ldquo;Problem\u0026rdquo; A common issue/request about Dropbox was about ignoring files without using Selective Sync. Dropbox recently introduced the ability to ignore specific files or folders. Although the significance of the new feature, many users [ 1, 2 , 3, 4] have expressed the importance of ignoring files from Dropbox using Glob just like .gitignore. For various reasons Dropbox doesn\u0026rsquo;t seem to have the will to implement such a feature :/\nThe walk-around As a result, I decided to implement dropboxignore which is a simple shell script that ignores files from Dropbox based on glob patterns. Additionally, existing .gitignore files can be used to automatically generate .dropboxignore files.\nThe main difference between dropboxignore and the other relevant projects:\n rozbb/DropboxIgnore swapagarwal/dropbox_ignore ridvanaltun/dropbox-ignore-anywhere MichalKarol/dropboxignore mweirauch/dropignore  is the fact that dropboxignore is implemented in shell and has the minimum requirements (attr package is the only requirement)\ndropboxignore is currently available only for Mac and Linux and has various features:\n Ignore/Revert specific files Automatically update .dropboxignore Revert all ignored files Updated .dropboxignore based on changes in .gitignore file. Generate .dropboxignore files based on existing .gitignore files. Generate, update and ignore using a single command. Delete all .dropboxignore files.  A usecase The simplest usage example could be this: Let\u0026rsquo;s say that you want to ignore every .txt under your Dropbox folder, in that case you only have to run the following command:\n$ dropboxignore ignore *.txt You can find more usage examples here. The full documentation of the project is available here\nThings to not expect from dropboxignore  Automatically ignore new matching files.  When a new file has been created and is matched by a .dropboxignore file, user should re-ignore matching files. A cronjob could solve this drawback.\nAutomatically detect deletions from .gitignore files  Current implementation can detect and update .dropboxignore file if an extra pattern has been added in the corresponding .gitignores but it can not handle deletions in the same way.\n","permalink":"https://simakis.me/posts/exclude-files-using-dropboxignore/","summary":"The \u0026ldquo;Problem\u0026rdquo; A common issue/request about Dropbox was about ignoring files without using Selective Sync. Dropbox recently introduced the ability to ignore specific files or folders. Although the significance of the new feature, many users [ 1, 2 , 3, 4] have expressed the importance of ignoring files from Dropbox using Glob just like .gitignore. For various reasons Dropbox doesn\u0026rsquo;t seem to have the will to implement such a feature :/","title":"Exclude files from your dropbox easily using dropboxignore"},{"content":"Introduction What is the problem and how deal with it When it comes to image storing, a common pitfall is to save all the images in a single folder. If the number of images is less than few thousands, when, stop reading this post because you will not face any issue. On the other hand, if you are planing to store numerous images, then consider splitting them in different folders. Listing a directory will become faster, more efficient and at the end of the day, your kernel will be happier. A common pattern is to create a folder structure based on the name of every file. For example, let’s say that path/to/image/dir will be the main directory, and you want to store imagefile.jpg. Create folder structure based on file’s characters and save the file inside the leaf folder:\n$ tree path/to/image/dir path/to/image/dir └── i └── m └── a └── imagefile.jpg A testcase Given the following situation:\n Folder structure’s depth is 3 The maximum number of images per leaf folder is 1000 A common hashing function is used for naming ([a-z0-9])  The main folder can host up to:\n$$\\left ( \\left ( 26 + 10 \\right )^3 \\right ) * 1000 \\approx 46 \\text{ Milion images}$$\nApplication Time to get our hands dirty.\nDescription The purpose of this post is demonstrate an easy way to apply this methodology using scrapy and specifically the ImagePipeline. The default behavior of ImagePipeline is to store all images in the same folder based on IMAGES_STORE‘s value in settings.py. We are going to make an ImagePipeline sub-class and we will override file_path and thumb_path methods. Please find below the full pipeline:\nImplementation  ","permalink":"https://simakis.me/posts/scrapy-store-images-efficiently-using-folder-structure/","summary":"Introduction What is the problem and how deal with it When it comes to image storing, a common pitfall is to save all the images in a single folder. If the number of images is less than few thousands, when, stop reading this post because you will not face any issue. On the other hand, if you are planing to store numerous images, then consider splitting them in different folders. Listing a directory will become faster, more efficient and at the end of the day, your kernel will be happier.","title":"Store images efficiently in scrapy using folder structure"},{"content":" ","permalink":"https://simakis.me/posts/semi-supervised-fraud-detection/","summary":" ","title":"Semi-Supervised Fraud Detection"},{"content":"kaggle dataset: Book Depository Dataset\nkaggle notebook: Introduction to Book Depository Dataset\ngithub repo: book-depository-dataset\nBook Depository Dataset EDA Through this notebook we will try to become familiar Book Depository Dataset and extract some usefull insights. The goal of this notebook is to become an introductory step for the dataset.\nimport pandas as pd import os import json from glob import glob import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline Dataset Structure Files:\n categories.csv dataset.csv formats.csv places.csv  The dataset consists of 5 file, the main dataset.csv file and some extra files. Extra files works as lookup tables for category, author, format and publication place. The reason behind this decision was to prevent data redundancy.\nFields:\n authors: Book\u0026rsquo;s author(s) (list of str) bestsellers-rank: Bestsellers ranking (int) categories: Book\u0026rsquo;s categories. Check authors.csv for mapping (list of int) description: Book description (str) dimension_x: Book\u0026rsquo;s dimension X (float cm) dimension_y: Book\u0026rsquo;s dimension Y (float cm) dimension_z: Book\u0026rsquo;s dimension Z (float mm) edition: Edition (str) edition-statement: Edition statement (str) for-ages: Range of ages (str) format: Book\u0026rsquo;s format. Check formats.csv for mapping (int) id: Book\u0026rsquo;s unique id (int) illustrations-note: imprint: index-date: Book\u0026rsquo;s crawling date (date) isbn10: Book\u0026rsquo;s ISBN-10 (str) isbn13: Book\u0026rsquo;s ISBN-13 (str) lang: List of book' language(s) publication-date: Publication date (date) publication-place: Publication place (id) publisher: Publisher (str) rating-avg: Rating average [0-5] (float) rating-count: Number of ratings title: Book\u0026rsquo;s title (str) url: Book relative url (https://bookdepository.com + url) weight: Book\u0026rsquo;s weight (float gr)  So, lets assign each file to a different dataframe\nif os.path.exists(\u0026#39;../input/book-depository-dataset\u0026#39;): path_prefix = \u0026#39;../input/book-depository-dataset/{}.csv\u0026#39; else: path_prefix = \u0026#39;../export/kaggle/{}.csv\u0026#39; df, df_f, df_a, df_c, df_p = [ pd.read_csv(path_prefix.format(_)) for _ in (\u0026#39;dataset\u0026#39;, \u0026#39;formats\u0026#39;, \u0026#39;authors\u0026#39;, \u0026#39;categories\u0026#39;, \u0026#39;places\u0026#39;) ] # df = df.sample(n=500) df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  authors bestsellers-rank categories description dimension-x dimension-y dimension-z edition edition-statement for-ages ... isbn10 isbn13 lang publication-date publication-place rating-avg rating-count title url weight     0 [1] 57858 [220, 233, 237, 2644, 2679, 2689] They were American and British air force offic... 142.0 211.0 20.0 NaN Reissue NaN ... 393325792 9.780393e+12 en 2004-08-17 1.0 4.24 6688.0 The Great Escape /Great-Escape-Paul-Brickhill/9780393325799 243.00   1 [2, 3] 114465 [235, 3386] John Moran and Carl Williams were the two bigg... 127.0 203.2 25.4 NaN NaN NaN ... 184454737X 9.781845e+12 en 2009-03-13 2.0 3.59 291.0 Underbelly : The Gangland War /Underbelly-Andrew-Rule/9781844547371 285.76   2 [4] 61,471 [241, 245, 247, 249, 378] Plain English is the art of writing clearly, c... 136.0 195.0 16.0 Revised 4th Revised edition NaN ... 199669171 9.780200e+12 en 2013-09-15 3.0 4.18 128.0 Oxford Guide to Plain English /Oxford-Guide-Plain-English-Martin-Cutts/97801... 338.00   3 [5] 1,347,994 [245, 253, 263, 273, 274, 276, 279, 280, 281, ... When travelling, do you want to journey off th... 136.0 190.0 33.0 Unabridged Unabridged edition NaN ... 1444185497 9.781444e+12 en 2014-12-03 2.0 NaN NaN Get Talking and Keep Talking Portuguese Total ... /Get-Talking-Keep-Talking-Portuguese-Total-Aud... 156.00   4 [6] 58154 [1938, 1941, 1995] No matter what your actual job title, you are-... 179.0 229.0 18.0 NaN NaN NaN ... 321934075 9.780322e+12 en 2016-02-28 4.0 4.30 212.0 The Truthful Art : Data, Charts, and Maps for ... /Truthful-Art-Alberto-Cairo/9780321934079 732.00    5 rows × 25 columns\n Basic Stats Firtly, lets display some basic statistics:\ndf.describe()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  dimension-x dimension-y dimension-z id isbn13 publication-place rating-avg rating-count weight     count 742112.000000 713278.000000 742112.000000 7.790050e+05 7.658780e+05 556846.000000 502381.000000 5.023810e+05 714289.000000   mean 160.560373 222.289753 25.609538 9.781553e+12 9.781559e+12 247.989972 3.932002 1.187949e+04 444.768939   std 37.487785 43.145377 44.218401 1.563374e+09 1.565216e+09 643.253808 0.530740 1.174093e+05 610.212039   min 0.250000 1.000000 0.130000 9.771131e+12 9.780000e+12 1.000000 1.000000 1.000000e+00 15.000000   25% 135.000000 198.000000 9.140000 9.780764e+12 9.780772e+12 2.000000 3.690000 6.000000e+00 172.370000   50% 152.000000 229.000000 16.000000 9.781473e+12 9.781475e+12 8.000000 4.000000 5.200000e+01 299.000000   75% 183.000000 240.000000 25.000000 9.781723e+12 9.781724e+12 178.000000 4.220000 6.880000e+02 521.630000   max 1905.000000 1980.000000 1750.000000 9.798485e+12 9.798389e+12 5501.000000 5.000000 5.870281e+06 90717.530000     Publication Date Distribution: Most books are published in t\ndf[\u0026#34;publication-date\u0026#34;] = df[\u0026#34;publication-date\u0026#34;].astype(\u0026#34;datetime64\u0026#34;) df.groupby(df[\u0026#34;publication-date\u0026#34;].dt.year).id.count().plot(title=\u0026#39;Publication date distribution\u0026#39;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7827af68d0\u0026gt;  df[\u0026#34;index-date\u0026#34;] = df[\u0026#34;index-date\u0026#34;].astype(\u0026#34;datetime64\u0026#34;) df.groupby(df[\u0026#34;index-date\u0026#34;].dt.month).id.count().plot(title=\u0026#39;Crawling date distribution\u0026#39;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7827af61d0\u0026gt;  df.groupby([\u0026#39;lang\u0026#39;]).id.count().sort_values(ascending=False)[:5].plot(kind=\u0026#39;pie\u0026#39;, title=\u0026#34;Most common languages\u0026#34;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f78279aca58\u0026gt;  import math sns.lineplot(data=df.groupby(df[\u0026#39;rating-avg\u0026#39;].dropna().apply(int)).id.count().reset_index(), x=\u0026#39;rating-avg\u0026#39;, y=\u0026#39;id\u0026#39;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7827970dd8\u0026gt;  dims = pd.DataFrame({ \u0026#39;dims\u0026#39;: df[\u0026#39;dimension-x\u0026#39;].fillna(\u0026#39;0\u0026#39;).astype(int).astype(str).str.cat( df[\u0026#39;dimension-y\u0026#39;].fillna(\u0026#39;0\u0026#39;).astype(int).astype(str), sep=\u0026#34; x \u0026#34;).replace(\u0026#39;0 x 0\u0026#39;, \u0026#39;Unknown\u0026#39;).values, \u0026#39;id\u0026#39;: df[\u0026#39;id\u0026#39;].values }) dims.groupby([\u0026#39;dims\u0026#39;]).id.count().sort_values(ascending=False)[:8].plot(kind=\u0026#39;pie\u0026#39;, title=\u0026#34;Most common dimensions\u0026#34;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f77ee8a2b38\u0026gt;  pd.merge( df[[\u0026#39;id\u0026#39;, \u0026#39;publication-place\u0026#39;]], df_p, left_on=\u0026#39;publication-place\u0026#39;, right_on=\u0026#39;place_id\u0026#39; ).groupby([\u0026#39;place_name\u0026#39;]).id.count().sort_values(ascending=False)[:8].plot(kind=\u0026#39;pie\u0026#39;, title=\u0026#34;Most common publication places\u0026#34;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f77ee96a208\u0026gt;  ","permalink":"https://simakis.me/posts/introduction-to-book-depository-dataset/","summary":"kaggle dataset: Book Depository Dataset\nkaggle notebook: Introduction to Book Depository Dataset\ngithub repo: book-depository-dataset\nBook Depository Dataset EDA Through this notebook we will try to become familiar Book Depository Dataset and extract some usefull insights. The goal of this notebook is to become an introductory step for the dataset.\nimport pandas as pd import os import json from glob import glob import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline Dataset Structure Files:","title":"Book Depository Dataset EDA"},{"content":"           1/2022 Pulsar API Essentials - Python cert StreamNative   1/2022 Apache Pulsar Fundamentals cert StreamNative   1/2022 Introduction to Kubeflow: Notebooks cert Arrikto   1/2022 Open Source Software Development, Linux and Git Specialization cert Coursera   12/2021 Architecting with Google Kubernetes Engine Specialization cert Coursera   12/2021 Introduction to Kubeflow: Fundamentals cert Arrikto   12/2021 Introduction to Machine Learning in Production cert Coursera   10/2020 AWS Fundamentals Specialization cert Coursera   4/2020 Programming Foundations: Web Security cert LinkedIn Learning   2/2020 Applied Text Mining in Python cert Coursera   2/2020 Sequences, Time Series and Prediction cert Coursera   3/2019 Oπe\\n conference cert Athens, Greece   12/2017 Spark and Python for Big Data with PySpark cert Udemy   11/2017 2th DevOps Conference 2017 cert Athens, Greece   11/2017 0th Conference of Free \u0026amp; Open Source Software Communities  Athens, Greece   4/2017 Solr 101 cert Cognitive Class   5/2011 4th Conference of Free \u0026amp; Open Source Software Communities  Patras, Greece    ","permalink":"https://simakis.me/certifications/","summary":"           1/2022 Pulsar API Essentials - Python cert StreamNative   1/2022 Apache Pulsar Fundamentals cert StreamNative   1/2022 Introduction to Kubeflow: Notebooks cert Arrikto   1/2022 Open Source Software Development, Linux and Git Specialization cert Coursera   12/2021 Architecting with Google Kubernetes Engine Specialization cert Coursera   12/2021 Introduction to Kubeflow: Fundamentals cert Arrikto   12/2021 Introduction to Machine Learning in Production cert Coursera   10/2020 AWS Fundamentals Specialization cert Coursera   4/2020 Programming Foundations: Web Security cert LinkedIn Learning   2/2020 Applied Text Mining in Python cert Coursera   2/2020 Sequences, Time Series and Prediction cert Coursera   3/2019 Oπe\\n conference cert Athens, Greece   12/2017 Spark and Python for Big Data with PySpark cert Udemy   11/2017 2th DevOps Conference 2017 cert Athens, Greece   11/2017 0th Conference of Free \u0026amp; Open Source Software Communities  Athens, Greece   4/2017 Solr 101 cert Cognitive Class   5/2011 4th Conference of Free \u0026amp; Open Source Software Communities  Patras, Greece    ","title":"Certifications \u0026 Conferences"},{"content":"Feel free to drop a line using the following contact form.\n Send 🚀  ","permalink":"https://simakis.me/contact/","summary":"Feel free to drop a line using the following contact form.\n Send 🚀  ","title":"Contact form"},{"content":"Summary Software Engineer with more than 5 years of working experience in the ﬁeld of Software Development, Data Science, Data Engineering \u0026amp; MLOps. Has worked in various sectors from Digital Marketing to e-Commerce. Loves to work with Modern Data Stacks and to productize Machine Learning Systems. He usually contributes to FOSS projects during the nights.\nExperience   Netdata \u0026ndash; Senior Data Engineer \u0026ndash; Oct. 2021 - Present\nDevelopment of ETL pipelines.\nTechnologies: Airflow, BigQuery, GCP, Docker\n  Convert Group \u0026ndash; Data Engineer \u0026ndash; May 2020 - Sep. 2021 \u0026ndash; Athens, Greece\nDevelopment of ETL pipelines and data automation pipelines. Productize the inernal Machine Learning services. Contribute on back-end development of eRetail Content platform.\nTechnologies: Prefect, AWS, Django/REST, Docker, PostgreSQL, Amazon Redshift, Celery\n  XPLAIN \u0026ndash; Data Scientist \u0026ndash; Feb. 2017 - Apr. 2020 \u0026ndash; Athens, Greece Data and Text Mining using state-of-the-art and disruptive methods. Design and development of distributed web crawlers. Database administration and optimization. Implementation of workflow pipelines. Deployment of ML models to production as RESTfull API services.\nTechnologies: Python, MySQL, Celery, Docker Swarm, ELK, spaCy, NumPy/pandas\n  Sociality \u0026ndash; Junior Web Developer \u0026ndash; Intern \u0026ndash; Feb. 2016 - June 2016 \u0026ndash; Athens, Greece\nWeb development using WordPress. Implementation of WordPress plugins for Woocommerce. Development of custom functionalities for WordPress.\nTechnologies: WordPress, PHP, MySQL, JavaScript, HTML/CSS\n  Technical Skills   Programming \u0026amp; Markup Languages: Python, Shell, SQL, PHP\n  Databases \u0026amp; Big Data: MySQL, Redis, PostgreSQL, Elastic Stack\n  Data \u0026amp; ML Workflow Platforms: Prefect, MLflow, Apache Airflow\n  Data Science \u0026amp; NLP: pandas/NumPy, scikit-learn, Keras, spaCy\n  Web Scraping: lxml, Selenium, Scrapy, Beautiful Soup\n  Web Frameworks: Django/REST, FastAPI\n  Version Control Systems \u0026amp; Platforms: Git, Github, Bitbucket\n  Education   Department of Computer Engineering \u0026amp; Informatics, University of Patras \u0026ndash; Sep. 2010 \u0026ndash; Feb. 2017 \u0026ndash; Patras, Greece\nDiploma (equivalent to B.Eng + M.Eng); GPA: 6.87\nThesis: Design and Implementation of a Methodology for the Automatic Identification of the User Geographic Idiom in a Social Media Text Corpus. supervisor: Megalooikonomou Vasileios, grade: 10/10\n  Languages  Greek: Native English: Proficient Certificate of Proficiency in English, The University of Mishigan French: Elementary Delf A2  Certifications \u0026amp; Conferences   Architecting with Google Kubernetes Engine Specialization: Coursera cert Dec. 2021\n  AWS Fundamentals Specialization: Coursera cert Nov. 2020\n  Oπe\\n conference: Athens, Greece cert Mar. 2019\n  Spark and Python for Big Data with PySpark: Udemy cert Dec. 2017\n  4th \u0026amp; 10th Conference of FOSS: Patras, Athens, Greece 2011, 2017\n  the full list\nPublications   Simaki, V., Simakis, P., Paradis, C., \u0026amp; Kerren, A. (2018). Detection of StanceRelated Characteristics in Social Media Text. In SETN \u0026lsquo;18: 10th Hellenic Conference on Artificial Intelligence New York: Association for Computing Machinery, Inc. DOI: 10.1145/3200947.3201017\n  Simaki, V., Simakis, P., Paradis, C, \u0026amp; Kerren, A. (2017). Identifying the Authors\u0026rsquo; National Variety of English in Social Media Text. In Proceedings of RANLP 2017 - Recent Advances in Natural Language, ACL Anthology (in press)\n  Interests GNU/Linux, MLOps, NLP, F/OSS\nActivities Photography, Traveling, Hiking, Sailing\n Click here to download my CV.\n","permalink":"https://simakis.me/cv/","summary":"Summary Software Engineer with more than 5 years of working experience in the ﬁeld of Software Development, Data Science, Data Engineering \u0026amp; MLOps. Has worked in various sectors from Digital Marketing to e-Commerce. Loves to work with Modern Data Stacks and to productize Machine Learning Systems. He usually contributes to FOSS projects during the nights.\nExperience   Netdata \u0026ndash; Senior Data Engineer \u0026ndash; Oct. 2021 - Present\nDevelopment of ETL pipelines.","title":"Curriculum Vitae"},{"content":" nltk/nltk: PR #2910 · Fix FileNotFoundError when the download_dir is a non-existing nested folder jazzband/django-auditlog: PR #295 · Replace deprecated smart_text() with smart_str() PrefectHQ/prefect:  PR #3729 · Support passing helper_script to ShellTask/DBTShellTask at runtime PR #3612 · pass day_or croniter argument to CronClock \u0026amp; CronSchedule PR #3548 · RenameFlowRunTask: use default flow_run_id value from context PR #3522 · specify slack webhook_secret on runtime PR #3479 · explicit exception chaining for spacy tasks PR #3460 · S3List filtering using the LastModified  PR #3072 · Ignore calls to flow.register when parsing a flow using file based storage PR #2924 · Fix ValueError on flow.replace   boudinfl/pke: PR #121 · disable unnecessary spacy pipeline components dldl/sphinx-server: PR #35 · Support Python 3 bisohns/search-engine-parser:  PR #64 · Added Coursera Search Engine PR #52 · Added Google Scholar   erichulser/mkdocs_autodoc: PR #3 · add missing dependency twingly/twingly-search-api-python: PR #40 · Add all required modules in setup.py lorien/awesome-web-scraping:  PR #65 · add rq PR #64 · add Requestium PR #40 · add grequests   lk-geimfari/expynent:  PR #69 · add pattern and test for greek license plates PR #57 · Pattern for greek phones   nltk/nltk_data: PR #103 · Add Greek Keywords riseupnet/riseup_help:  PR #411 · \u0026ldquo;Riseup chat\u0026rdquo; has been translated to greek PR #403 · Greek translation for the home page    ","permalink":"https://simakis.me/contributions/","summary":"nltk/nltk: PR #2910 · Fix FileNotFoundError when the download_dir is a non-existing nested folder jazzband/django-auditlog: PR #295 · Replace deprecated smart_text() with smart_str() PrefectHQ/prefect:  PR #3729 · Support passing helper_script to ShellTask/DBTShellTask at runtime PR #3612 · pass day_or croniter argument to CronClock \u0026amp; CronSchedule PR #3548 · RenameFlowRunTask: use default flow_run_id value from context PR #3522 · specify slack webhook_secret on runtime PR #3479 · explicit exception chaining for spacy tasks PR #3460 · S3List filtering using the LastModified  PR #3072 · Ignore calls to flow.","title":"My F/OSS Contributions"}]